---
title: "Probability Distributions"
author: "Robert W. Walker"
date: "2020-02-12"
output: 
  html_document:
    code_download: true  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning=FALSE)
```


## Probability: The Logic of Science

Jaynes presents a few core ideas and requirements for his rational system.  Probability emerges as the representation of circumstances in which any given realization of a process is either TRUE or FALSE but both are possible and expressable by probabilities

- that sum to one for all events
- are greater than or equal to zero for any given event

***

## General Representation of Probability

Is of necessity two-dimensional,

- We have $x$ and 
- we have $Pr(X=x)$ in one of two forms (Pr or f).


***

```{r P1, echo=FALSE}
plot(x=NA, y=NA, xlab="x", ylab="Pr(X=x)", xlim=c(-5,5), ylim=c(0,1), bty="n", main="A General Probability Distribution")
```

***

## Probability Distributions of Two Forms

Our core concept is a probability distribution just as above.  These come in two forms for two types [discrete (qualitative)] and continuous (quantitative)]:


- Assumed
- Derived

***

## [The Poster](https://github.com/robertwwalker/DADMStuff/raw/master/Distribution-Poster.pdf) and Examples

Distributions are nouns.  Sentences are incomplete without verbs -- parameters.  We need both; it is for this reason that the former slide is true.  We do not always have a grounding for either the name or the parameter.

***

## Continuous vs. Discrete Distributions

The differences are sums versus integrals.  Why?  

- Histograms or  
- Density Plots  

The probability of exactly any given value is zero on a true continuum.

***

Expectation

$$E(X) = \sum_{x \in X} x \cdot Pr(X=x)$$
$$E(X) = \int_{x \in X} x \cdot f(x)dx$$

Variance

$$E[(X-\mu)^2] = \sum_{x \in X} (x-\mu)^2 \cdot Pr(X=x)$$
$$E((X-\mu)^2) = \int_{x \in X} (x-\mu)^2 \cdot f(x)dx$$


# The z-transform

The generic z-transformation applied to a variable $x$ centers [mean$\approx$ 0] and scales [std. dev. $\approx$ variance $\approx$ 1] to $z_{x}$ for population parameters.  $\approx$ is approximately equal to.

$$ z = \frac{x - \mu}{\sigma} $$

***

In samples, the 0 and 1 are exact; these are features of the mean and *degrees of freedom* from last time.

$$ z = \frac{x - \overline{x}}{s_{x}} $$.

where $\overline{x}$ is the sample mean of $x$ and $s_{x}$ is the sample standard deviation of $x$.  Take the example of earnings.  

***

Suppose earnings in a community have mean 55,000 and standard deviation 10,000.  This is in dollars.  Suppose I earn 75,000 dollars.  First, if we take the top part of the fraction in the $z$ equation, we see that I earn 20,000 dollars more than the average (75000 - 55000).  Finishing the calculation of z, I would divide that 20,000 dollars by 10,000 dollars per standard deviation.  Let's show that.

$$ z = \frac{75000 dollars - 55000 dollars}{\frac{10000 dollars}{SD}} = +2 SD $$.

I am 2 standard deviations above the average (the +) earnings.  All $z$ does is re-scale the original data to standard deviations with zero as the mean.  

***

Suppose I earn 35,000.  That makes me 20,000 below the average and gives me a z score of -2.  I am 2 standard deviations below average (the -) earnings.

$z$ is an easy way to assess symmetry.  The mean of z is always zero but the distribution of z to the left and right of zero is informative.  If they are roughly even, then symmetry is likely.  If the signs are uneven, then symmetry is unlikely.  In R, $z$ is automated with the *scale()* command.  The last line uses a table and the *sign* command to show me the positive and negative z.

***

```{r SimD1}
# Generate random normal income
Hypo.Income <- rnorm(1000, 55000, 10000)
# z-transform income [mean 55000ish, std. dev. 10000ish]
z.Income <- scale(Hypo.Income)
# Combine them into a data.frame
Income <- data.frame(Hypo.Income,z.Income)
# Show the data.frame
head(Income)
table(sign(z.Income))
```



# Probability Distributions

Distributions in R are defined by four core parts: 

- r, for random variables
- p, for cumulative probability (given x) [counting from left] $Pr(X\leq x)$
- d, for density/probability that $Pr(X=x)$ or $f(x)$
- q, for quantile (given p): x such that $Pr(X\leq q)=p$


## A Grape Escape?

A filling process is supposed to fill jars with 16 ounces of grape jelly, according to the label, and regulations require that each jar contain between 15.95 and 16.05 ounces.

1.	*Suppose that the uniform random process of filling in known to fill between 15.9 and 16.1 ounces uniformly.*
+ Plot the histogram of 1000 simulated values.  NB: *unif* is the noun with boundaries a (default 0) and b(default 1). 

```{r}
Jars <- runif(1000, 15.9, 16.1)
hist(Jars)
```

+ What is the probability that a random jar is outside of requirements?

**Exactly?  50 percent because 25 percent are between 15.9 and 15.95 and 25 percent are between 16.05 and 16.1.**

```{r}
table(Jars < 15.95 | Jars > 16.05) # | captures or
```


+ Scale (z) the jars and summarise them.

```{r}
summary(scale(Jars))
sd(scale(Jars))
```

***

2.	*The mean of the normal random process of filling is known to be 16.004 ounces with standard deviation 0.028 ounces.*
+ What is the probability that a random jar is outside of requirements?  NB: *norm* is the noun with mean (default 0) and sd (default 1).

```{r}
pnorm(15.95, 16.004, 0.028) + pnorm(16.05, 16.004, 0.028, lower.tail=FALSE)
```


+ What is the probability that a random jar contains more than 16.1 ounces?

```{r}
1-pnorm(16.1, 16.004, 0.028)
```


+ What is the probability that a random jar contains less than 16.04 ounces?

```{r}
pnorm(16.04, 16.004, 0.028)
```

+ 95% of jars, given a normal, will contain between XXX and XXX ounces of jelly.

```{r}
qnorm(c(0.025,0.975), 16.004, 0.028)
```

+ The bottom 5% of jars contain, at most, XXX ounces of jelly.

```{r}
qnorm(0.05, 16.004, 0.028)
```

+ The top 25% of jars contain at least XXX ounces of jelly.

```{r}
qnorm(0.75, 16.004, 0.028)
```


## Scottish Pounds

*Informal surveys suggest that 15% of Essex shopkeepers will not accept Scottish pounds.  There are approximately 200 shops in the general High Street square.*

1. Draw a plot of the distribution and the cumulative distribution of shopkeepers that do not accept Scottish pounds.

```{r}
plot(x=seq(0,200), y=pbinom(seq(0,200), size=200, 0.15), xlab="Refusers", ylab="Prob. of x or Less Refusers")
```

2. What is the probability that 24 or fewer will not accept Scottish pounds?

```{r}
pbinom(24, 200, 0.15)
```


3. What is the probability that 25 or more shopkeepers will not accept Scottish pounds?

```{r}
1-pbinom(24, 200, 0.15)
```

4. With probability 0.9 [90 percent], XXX or fewer shopkeepers will not accept Scottish pounds.

```{r}
qbinom(0.9, 200, 0.15)
```

5. [Geometric] Plot 1000 random draws of "How many vendors until one refuses my Scottish pounds?"

```{r}
hist(rgeom(1000, 0.15), breaks=30)
```

We could also do something like.

```{r}
plot(seq(0,60), pgeom(seq(0,60), 0.15))
```


## The Median is a Binomial with p=0.5

Interestingly, any given observation has a 50-50 chance of being *over* or *under* the median.  Suppose that I have five datum.  
1. What is the probability that all are under?

```{r}
pbinom(0,size=5, p=0.5)
```


2. What is the probability that all are over?

```{r}
dbinom(5,size=5, p=0.5)
```

3. What is the probability that the median is somewhere in between our smallest and largest sampled values?

**Everything else**.

This *Rule of Five* often credited to Hubbard is handy.

### Air Traffic Controllers

FAA Decision: Expend or do not expend scarce resources investigating claimed staffing shortages at the Cleveland Air Route Traffic Control Center.

Essential facts: The Cleveland ARTCC is the US's busiest in routing cross-country air traffic.  In mid-August of 1998, it was reported that the first week of August experienced 3 errors in a one week period; an error occurs when flights come within five miles of one another by horizontal distance or 2000 feet by vertical distance.  The Controller's union claims a staffing shortage though other factors could be responsible.  21 errors per year (21/52 errors per week) has been the norm in Cleveland for over a decade.  

1. Plot a histogram of 1000 random weeks.  NB: *pois* is the noun with no default for $\lambda$ -- the arrival rate.

```{r}
hist(rpois(1000, 21/52))
```


+ Plot a sequence on the x axis from 0 to 5 and the probability of that or fewer incidents along the y.  *seq(0,5)*

```{r}
plot(seq(0,5),ppois(seq(0,5), 21/52), ylim=c(0,1))
```


2. What would you do and why?  *Not impossible*

3. After analyzing the initial data, you discover that the first two weeks of August have experienced 6 errors.  What would you now decide?  *Well, once is `r ppois(2, 21/52, lower.tail=FALSE)`.*  Twice, at random, is that squared.  We have a problem.

### Deaths by Horse Kick in the Prussian cavalry?

### [Given time] A Less Basic Monte Carlo Simulation:

1.	Customers arriving at a car dealership at a rate of 6 per hour.
2.	Each customer has a 15% probability of making a purchase.
3.	Purchasers yield [this part is harder]: 
+	Uniform profits over the interval $1000-$3000.
+	Normal profits that average $1500 with standard deviation $500.

```{r MC1}
Customers <- rpois(1000, 6) # Customers ~ Poisson(6)
Purchasers <- rbinom(1000, size=Customers, prob=0.15) # P ~ Binomial(Customers,0.15)
# Next part needs a coding trick.  For each row [of 1000], I want sum the Profits given Purchasers random draws.
Profits.U <- sapply(c(1:1000), function(x) { sum(runif(Purchasers[[x]], 1000, 3000))} )
Profits.N <- sapply(c(1:1000), function(x) { sum(rnorm(Purchasers[[x]], 1500, 500))} )
plot(density(Profits.N))
```

