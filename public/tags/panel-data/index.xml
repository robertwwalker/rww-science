<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>panel data | rww.science</title>
    <link>/tags/panel-data/</link>
      <atom:link href="/tags/panel-data/index.xml" rel="self" type="application/rss+xml" />
    <description>panel data</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2018</copyright><lastBuildDate>Thu, 11 Apr 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/portrait.jpg</url>
      <title>panel data</title>
      <link>/tags/panel-data/</link>
    </image>
    
    <item>
      <title>fredr is very neat</title>
      <link>/2019/04/11/fredr-is-very-neat/</link>
      <pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/2019/04/11/fredr-is-very-neat/</guid>
      <description>


&lt;div id=&#34;fred-via-fredr&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;FRED via &lt;code&gt;fredr&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;The Federal Reserve Economic Database [FRED] is a wonderful public resource for data and the r api that connects to it is very easy to use for the things that I have previously needed. For example, one of my students was interested in commercial credit default data. I used the FRED search instructions from the following vignette to find that data. My first step was the &lt;a href=&#34;https://cran.r-project.org/web/packages/fredr/vignettes/fredr.html&#34;&gt;vignette for using &lt;code&gt;fredr&lt;/code&gt;&lt;/a&gt;. Some library lines give me tools.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(fredr); library(purrr)
library(dplyr)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dplyr&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:stats&amp;#39;:
## 
##     filter, lag&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following objects are masked from &amp;#39;package:base&amp;#39;:
## 
##     intersect, setdiff, setequal, union&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A student wanted to find data on charge-offs. &lt;code&gt;fredr&lt;/code&gt; has a search capability. Let’s see what we can find.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ChargeOff &amp;lt;- fredr_series_search_text(
  search_text = &amp;quot;real estate charge offs&amp;quot;,
  order_by = &amp;quot;popularity&amp;quot;,
  sort_order = &amp;quot;desc&amp;quot;)
ChargeOff %&amp;gt;% select(title)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 24 x 1
##    title                                                                        
##    &amp;lt;chr&amp;gt;                                                                        
##  1 Charge-Off Rate on Commercial Real Estate Loans (Excluding Farmland), Booked…
##  2 Charge-Off Rate on Loans Secured by Real Estate, All Commercial Banks        
##  3 Charge-Off Rate on Commercial Real Estate Loans (Excluding Farmland), Booked…
##  4 Charge-Off Rate on Commercial Real Estate Loans (Excluding Farmland), Booked…
##  5 Net Charge-Offs on All Loans and Leases, Secured by Real Estate, Single Fami…
##  6 Charge-Off Rate on Loans Secured by Real Estate, Banks Not Among the 100 Lar…
##  7 Charge-Off Rate on Commercial Real Estate Loans (Excluding Farmland), Booked…
##  8 Charge-Off Rate on Loans Secured by Real Estate, Top 100 Banks Ranked by Ass…
##  9 Net Charge-Offs on All Loans and Leases, Secured by Real Estate, Commercial …
## 10 Net Charge-Offs on All Loans and Leases, Secured by Real Estate, Single Fami…
## # … with 14 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wow, there is a rich array of data from that query. An even cooler feature is the ability to retrieve multiple at once in combination with the &lt;code&gt;map_dfr&lt;/code&gt; command from the &lt;code&gt;purrr&lt;/code&gt; library to make it tidy; that will go through the search results and stack the data that is returned [bind it together by rows]. These map commands are the tidy version of apply. In this case, binding the columns will likely fail because the times series are unlikely to be of the same length and that would create ragged columns. Better tidy from the start here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(stringr)
ChargeOff.Data &amp;lt;- map_dfr(ChargeOff$id, fredr) %&amp;gt;% left_join(ChargeOff, by=c(&amp;quot;series_id&amp;quot; = &amp;quot;id&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Just to finish one piece, let me show what these data look like for one series. Splitting the name into two parts gives us the title and subtitle but the split is inconsistent so the solution is not general.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ChargeOff.Short &amp;lt;- ChargeOff.Data %&amp;gt;% filter(series_id==&amp;quot;CORSREOBS&amp;quot;) 
SeriesTitle &amp;lt;- ChargeOff.Short[1,&amp;quot;title&amp;quot;] %&amp;gt;% str_split(., &amp;quot;[,]&amp;quot;) %&amp;gt;% unlist()
ChargeOff.Short %&amp;gt;% ggplot(data = ., mapping = aes(x = date, y = value)) +
    geom_line() +
    labs(x = &amp;quot;Date&amp;quot;, y = &amp;quot;Rate&amp;quot;, title=SeriesTitle[[1]], subtitle = SeriesTitle[[2]] )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-11-fredr-is-very-neat/index_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;counties-and-mapping&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Counties and Mapping&lt;/h2&gt;
&lt;p&gt;When I was setting up &lt;code&gt;fredr&lt;/code&gt;, I came across some data on subprime credit percentages. The data that I need for what I want to do is not obvious from the vignette and it turns out that this data is stored in a rather inconvenient fashion.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##   id                
##   &amp;lt;chr&amp;gt;             
## 1 EQFXSUBPRIME036061&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The way that these are organized in FRED is going to make this a mess. The series_id actually contains data. Every data vector is stored as the series name and a FIPS code embdeded. The FIPS code is likely to be the last five characters/numbers. Fun. My other limitation is that I am only seeing 1000 of 3000+ counties. That’s a limit of the API. I am going to have to do this a bit differently. I am going to work backward. I have access to the full set of county fips codes so I think that I can build the dataset to query. Here goes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(readr)
CFIPS &amp;lt;- read_delim(url(&amp;quot;https://github.com/robertwwalker/academic-mymod/raw/master/data/COUNTYFIPS.txt&amp;quot;), &amp;quot;\t&amp;quot;, escape_double = FALSE, trim_ws = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   FIPS = col_character(),
##   Name = col_character(),
##   State = col_character()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;head(CFIPS)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   FIPS  Name    State
##   &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;   &amp;lt;chr&amp;gt;
## 1 01001 Autauga AL   
## 2 01003 Baldwin AL   
## 3 01005 Barbour AL   
## 4 01007 Bibb    AL   
## 5 01009 Blount  AL   
## 6 01011 Bullock AL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That is all the FIPS codes that I will need. To create a new character vector for the data, it should just be a new column. From above, I know the names. Let me just concatenate the names to the FIPS code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;CFIPS$series_id &amp;lt;- as.character(paste0(&amp;quot;EQFXSUBPRIME0&amp;quot;,CFIPS$FIPS))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now for the moment of truth; I will only try this for Oregon.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;SubPrime.OR &amp;lt;- CFIPS %&amp;gt;% filter(State==&amp;quot;OR&amp;quot;) %&amp;gt;% select(series_id) %&amp;gt;% unlist() %&amp;gt;% map_dfr(., fredr)
SubPrime.OR &amp;lt;- left_join(SubPrime.OR, CFIPS, by=&amp;quot;series_id&amp;quot;)
SubPrime.OR %&amp;gt;% ggplot(aes(x=date, y=value, colour=series_id)) +
  geom_line() + theme(legend.position=&amp;quot;none&amp;quot;) + ggtitle(&amp;quot;The Subprime Credit Rate Across Oregon Counties&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-11-fredr-is-very-neat/index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now I have something to work with. Just to show what &lt;code&gt;gganimate&lt;/code&gt; can do; I will animate that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(gganimate)
library(ggrepel)
library(tidyr)
SubPrime.OR %&amp;gt;% ggplot(aes(x=date, y=value, colour=series_id)) +
  geom_line() + 
  geom_point(aes(group=series_id)) +
  geom_text_repel(aes(y = value, x = as.Date(&amp;quot;2019-01-01&amp;quot;), label = Name), hjust = 1, nudge_x = 8) + theme(legend.position=&amp;quot;none&amp;quot;) + labs(title=&amp;#39;The Subprime Credit Rate Across Oregon Counties: {frame_along}&amp;#39;, y =&amp;quot;Subprime Rate&amp;quot;) +
  transition_reveal(date)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-11-fredr-is-very-neat/index_files/figure-html/unnamed-chunk-6-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;My goal here is to build an animation of the map of this. So here goes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;states &amp;lt;- map_data(&amp;quot;state&amp;quot;)
OR.df &amp;lt;- subset(states, region == &amp;quot;oregon&amp;quot;)
OR_base &amp;lt;- ggplot(data = OR.df, mapping = aes(x = long, y = lat, group = group)) +
geom_polygon(color = &amp;quot;black&amp;quot;, fill = &amp;quot;gray&amp;quot;)+ theme_minimal() 
OR_base + geom_point(aes(x=-123.0433, y=44.925167))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-11-fredr-is-very-neat/index_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(emoGG)
counties &amp;lt;- map_data(&amp;quot;county&amp;quot;)
OR.county &amp;lt;- subset(counties, region == &amp;quot;oregon&amp;quot;)
OR.Map &amp;lt;- ggplot(data = OR.df, mapping = aes(x = long, y = lat, group = group)) + 
    geom_polygon(color = &amp;quot;black&amp;quot;, fill = &amp;quot;gray&amp;quot;)+ theme_minimal() 
OR_emoj &amp;lt;- OR_base +  add_emoji(&amp;quot;1f352&amp;quot;, x=-123.0433, y=44.925167, ysize=0.5)
OR_emoj&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-11-fredr-is-very-neat/index_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now to build a first map to make sure that it works.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(maps)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;maps&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:purrr&amp;#39;:
## 
##     map&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;county.fips$fips &amp;lt;- as.character(county.fips$fips)
SubPrime.M &amp;lt;- left_join(SubPrime.OR, county.fips, by=c(&amp;quot;FIPS&amp;quot;=&amp;quot;fips&amp;quot;))
SubPrime.M &amp;lt;- SubPrime.M %&amp;gt;% separate(., polyname, c(&amp;quot;region&amp;quot;,&amp;quot;subregion&amp;quot;), sep=&amp;quot;,&amp;quot;)
SubPrime.M %&amp;gt;% filter(date==&amp;quot;1999-01-01&amp;quot;) -&amp;gt; OneYear
OR.MD &amp;lt;- inner_join(OR.county, OneYear, by = &amp;quot;subregion&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One of my favortite bits of code from I do not know where:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ditch_the_axes &amp;lt;- theme(
  axis.text = element_blank(),
  axis.line = element_blank(),
  axis.ticks = element_blank(),
  panel.border = element_blank(),
  panel.grid = element_blank(),
  axis.title = element_blank()
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now to a single map.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Polt1 &amp;lt;- OR.Map + 
      geom_polygon(data = OR.MD, aes(fill = value), color=&amp;quot;white&amp;quot;) +
#      geom_polygon(color = &amp;quot;black&amp;quot;, fill = NA) +
      theme_bw() +
      ditch_the_axes +
      ggtitle(&amp;quot;Subprime Credit Percentages by County&amp;quot;)
Polt1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-11-fredr-is-very-neat/index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now for the general case. Same merge but with all the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;OR.MD2 &amp;lt;- left_join(SubPrime.M, OR.county, by = &amp;quot;subregion&amp;quot;)
OR.MD2 &amp;lt;- OR.MD2 %&amp;gt;% mutate(Subprime.Pct = value)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here goes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(viridis)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: viridisLite&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# ggplot(data = OR.MD2, mapping = aes(x = long, y = lat, group = group)) +
p &amp;lt;- OR.Map +
  geom_polygon(data=OR.MD2, aes(x = long, y = lat, group = subregion, fill = Subprime.Pct), color=&amp;quot;white&amp;quot;) +
  viridis::scale_fill_viridis(option=&amp;quot;C&amp;quot;) +   
  ditch_the_axes +
  labs(title = &amp;#39;Subprime Credit Percentage by Quarter: {closest_state}&amp;#39;) +
  transition_states(date, transition_length = 3, state_length = 3)
animate(p, nframes = 300)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-04-11-fredr-is-very-neat/index_files/figure-html/unnamed-chunk-13-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Pew Data on Bond Ratings and Rainy Day Funds</title>
      <link>/2018/03/07/pew-data-on-bond-ratings-and-rainy-day-funds/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      <guid>/2018/03/07/pew-data-on-bond-ratings-and-rainy-day-funds/</guid>
      <description>


&lt;div id=&#34;pew-on-rainy-day-funds-and-credit-quality&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Pew on Rainy Day Funds and Credit Quality&lt;/h1&gt;
&lt;p&gt;The Pew Charitable Trusts released a report last May (2017) that portrays rainy day funds that are well designed and deployed as a form of insurance against ratings downgrades. One the one hand, this is perfectly sensible because the alternatives do not sound like very good ideas. A poorly designed rainy day fund, for example, is going to have to fall short on either the rainy day or the fund. A poorly deployed savings device for cash flow management over the not-so-short term also seems unlikely to bolster market confidence in the repayment abilities of an issuer. If this very simple perspective that seems plausible is true, then a simple replication should be easy. And it is. Pew gladly shared the data and code. If one has access to Stata, the study is easy to replicate.&lt;/p&gt;
&lt;p&gt;Taken from the website above:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;../../img/PewCTRecs.png&#34; alt=&#34;Pew Recommendations&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Pew Recommendations&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;on-the-other-hand&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;On the other hand&lt;/h2&gt;
&lt;p&gt;The variation in the data may leave a good bit to be desired. Let’s have a look at some basic features of the data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(haven)
library(dplyr)
library(here)
Pew.Data &amp;lt;- read_dta(url(&amp;quot;https://github.com/robertwwalker/academic-mymod/raw/master/data/Pew/modeledforprediction.dta&amp;quot;))
glimpse(Pew.Data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 966
## Variables: 45
## $ fyear            &amp;lt;dbl&amp;gt; 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002…
## $ statefips        &amp;lt;dbl&amp;gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ state            &amp;lt;chr&amp;gt; &amp;quot;Alabama&amp;quot;, &amp;quot;Alabama&amp;quot;, &amp;quot;Alabama&amp;quot;, &amp;quot;Alabama&amp;quot;, &amp;quot;Alabama…
## $ bbalance         &amp;lt;dbl&amp;gt; 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000…
## $ withdraw         &amp;lt;dbl&amp;gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.00…
## $ deposit          &amp;lt;dbl&amp;gt; 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.0…
## $ interest         &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ ebalance         &amp;lt;dbl&amp;gt; 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000…
## $ fund             &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1…
## $ spo              &amp;lt;dbl&amp;gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, …
## $ moodyo           &amp;lt;dbl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0, 0, 0, 0, …
## $ fitcho           &amp;lt;dbl&amp;gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 0, 0…
## $ spnum            &amp;lt;dbl&amp;gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5…
## $ moodynum         &amp;lt;dbl&amp;gt; 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7…
## $ fitchnum         &amp;lt;dbl&amp;gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6…
## $ surplus          &amp;lt;dbl&amp;gt; -3, -73, 4, -35, 27, 21, 30, -34, -47, -177, 152, 35…
## $ gfebal           &amp;lt;dbl&amp;gt; 128, 54, 58, 23, 51, 72, 101, 67, 19, 113, 347, 664,…
## $ longdebt         &amp;lt;dbl&amp;gt; 0.10800536, 0.09275389, 0.08864151, 0.08145412, 0.08…
## $ shortdebt        &amp;lt;dbl&amp;gt; 2.790155e-05, 8.648518e-05, 7.688679e-06, 4.916201e-…
## $ totaldebt        &amp;lt;dbl&amp;gt; 0.10803326, 0.09284038, 0.08864920, 0.08145904, 0.08…
## $ population       &amp;lt;dbl&amp;gt; 8.346216, 8.357078, 8.365626, 8.373577, 8.382046, 8.…
## $ pop65            &amp;lt;dbl&amp;gt; 548.045, 554.718, 561.331, 567.094, 571.722, 574.279…
## $ gfe              &amp;lt;dbl&amp;gt; 3860, 4151, 4240, 4475, 4688, 4919, 5215, 5213, 5325…
## $ gfr              &amp;lt;dbl&amp;gt; 3857, 4078, 4244, 4440, 4715, 4940, 5245, 5179, 5278…
## $ spi              &amp;lt;dbl&amp;gt; 79368.02, 84194.33, 88048.51, 92206.95, 98699.30, 10…
## $ rdfbal           &amp;lt;dbl&amp;gt; 0.0000000, 0.0000000, 0.0000000, 0.0000000, 0.000000…
## $ rdfdep           &amp;lt;dbl&amp;gt; 0.00000, 0.00000, 0.00000, 0.00000, 0.00000, 0.00000…
## $ rdfwit           &amp;lt;dbl&amp;gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.…
## $ gfebalgfe        &amp;lt;dbl&amp;gt; 3.3160622, 1.3008914, 1.3679246, 0.5139665, 1.087883…
## $ spratingshift    &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ spupgrade        &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ spdowngrade      &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ moodyratingshift &amp;lt;dbl&amp;gt; NA, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,…
## $ moodyupgrade     &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, …
## $ moodydowngrade   &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ fitchratingshift &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …
## $ fitchupgrade     &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …
## $ fitchdowngrade   &amp;lt;dbl&amp;gt; NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …
## $ rdfnet           &amp;lt;dbl&amp;gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.00…
## $ rdfnetgfe        &amp;lt;dbl&amp;gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.…
## $ revenuebw        &amp;lt;dbl&amp;gt; 4.613832, 15.427544, -32.826214, -55.708824, 5.46266…
## $ revenuebwtrend   &amp;lt;dbl&amp;gt; 3852.386, 4062.573, 4276.826, 4495.709, 4709.537, 48…
## $ trendstanding    &amp;lt;dbl&amp;gt; 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -…
## $ valenceusage     &amp;lt;dbl&amp;gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.00…
## $ valenceusagegfe  &amp;lt;dbl&amp;gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000000, 0.…&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(Pew.Data$state)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##        Alabama         Alaska        Arizona       Arkansas     California 
##             21             21             21             21             21 
##    Connecticut       Delaware        Florida        Georgia         Hawaii 
##             21             21             21             21             21 
##          Idaho        Indiana           Iowa       Kentucky      Louisiana 
##             21             21             21             21             21 
##          Maine       Maryland  Massachusetts       Michigan      Minnesota 
##             21             21             21             21             21 
##    Mississippi       Missouri       Nebraska         Nevada  New Hampshire 
##             21             21             21             21             21 
##     New Jersey     New Mexico       New York North Carolina   North Dakota 
##             21             21             21             21             21 
##           Ohio       Oklahoma         Oregon   Pennsylvania   Rhode Island 
##             21             21             21             21             21 
## South Carolina   South Dakota      Tennessee          Texas           Utah 
##             21             21             21             21             21 
##        Vermont       Virginia     Washington  West Virginia      Wisconsin 
##             21             21             21             21             21 
##        Wyoming 
##             21&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The panel is balanced; in the original, New Mexico, New York, South Carolina, and Vermont are duplicated but the Stata code writes out a transformed dataset for analysis that is recorded. The technical report accompanying the study and the stata code give us some insights. In all cases, there are two or more RDF’s and they require combining.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;combining-ratings&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Combining Ratings&lt;/h1&gt;
&lt;p&gt;In previous work, Skip Krueger and I have treated bond ratings as a multiple rater problem and have deployed cumulative IRT models to measure latent credit quality. One of the methodologically desireable approaches to the Pew study was a model deploying state-level fixed effects but the ordinal data precludes doing this reliably because states that have always experienced the highest rating will have unbounded fixed effects. The continuous latent scale post measurement allows us to sidestep that problem. First, let me scale the data&lt;/p&gt;
&lt;div id=&#34;scaling-the-ratings&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Scaling the Ratings&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(MCMCpack)
Scaled.BR &amp;lt;- MCMCordfactanal(~spnum+fitchnum+moodynum, data=Pew.Data, factors=1, burnin = 1e6, mcmc=1e6, thin=100, store.scores=TRUE, tune=0.7, lambda.constraints=list(fitchnum=list(2,&amp;quot;+&amp;quot;)), verbose=50000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
load(url(&amp;quot;https://github.com/robertwwalker/academic-mymod/raw/master/data/Pew/Scaled-BR-Pew.RData&amp;quot;))
state.ratings &amp;lt;- data.frame(state=Pew.Data$state, statefips=Pew.Data$statefips, year=Pew.Data$fyear, BR.Data)
state.ratings.long &amp;lt;- tidyr::gather(state.ratings, sampleno, value, -statefips, -year, -state)
state.SE &amp;lt;- state.ratings.long %&amp;gt;% group_by(state,year) %&amp;gt;% summarise(Credit.Quality=mean(value), t1=quantile(value, probs=0.025), t2=quantile(value, probs=0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;what-does-the-scaling-look-like&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What does the scaling look like?&lt;/h2&gt;
&lt;div id=&#34;the-first-group&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The First Group&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stored &amp;lt;- list()
stored &amp;lt;- state.SE %&amp;gt;% group_by(state) %&amp;gt;% filter(state%in%c(names(table(state.SE$state))[c(1:16)])) %&amp;gt;%
  ggplot(., aes(x=year, y=Credit.Quality)) + theme_minimal() + theme(axis.text.x  = element_text(angle=60)) +
    geom_ribbon(aes(ymin=t1, ymax=t2, colour=state, fill=state), alpha=0.4) + guides(fill=&amp;quot;none&amp;quot;, alpha=&amp;quot;none&amp;quot;) +
    geom_line() + guides(colour=&amp;quot;none&amp;quot;) +
#    geom_jitter(width=0.2) +
#    geom_point(shape=21, size=3, fill=&amp;quot;white&amp;quot;) +
    ylim(-4,4) + facet_wrap(~state)
stored&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-03-07-pew-data-on-bond-ratings-and-rainy-day-funds/index_files/figure-html/Plot5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-second-group&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Second Group&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stored &amp;lt;- list()
stored &amp;lt;- state.SE %&amp;gt;% group_by(state) %&amp;gt;% filter(state%in%c(names(table(state.SE$state))[c(17:32)])) %&amp;gt;%
  ggplot(., aes(x=year, y=Credit.Quality)) + theme_minimal() + theme(axis.text.x  = element_text(angle=60)) +
    geom_ribbon(aes(ymin=t1, ymax=t2, colour=state, fill=state), alpha=0.4) + guides(fill=&amp;quot;none&amp;quot;, alpha=&amp;quot;none&amp;quot;) +
    geom_line() + guides(colour=&amp;quot;none&amp;quot;) +
#    geom_jitter(width=0.2) +
#    geom_point(shape=21, size=3, fill=&amp;quot;white&amp;quot;) +
    ylim(-4,4) + facet_wrap(~state)
stored&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-03-07-pew-data-on-bond-ratings-and-rainy-day-funds/index_files/figure-html/Plot5b-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-third-group&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Third Group&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stored &amp;lt;- list()
stored &amp;lt;- state.SE %&amp;gt;% group_by(state) %&amp;gt;% filter(state%in%c(names(table(state.SE$state))[c(33:46)])) %&amp;gt;%
  ggplot(., aes(x=year, y=Credit.Quality)) + theme_minimal() + theme(axis.text.x  = element_text(angle=60)) +
    geom_ribbon(aes(ymin=t1, ymax=t2, colour=state, fill=state), alpha=0.4) + guides(fill=&amp;quot;none&amp;quot;, alpha=&amp;quot;none&amp;quot;) +
    geom_line() + guides(colour=&amp;quot;none&amp;quot;) +
#    geom_jitter(width=0.2) +
#    geom_point(shape=21, size=3, fill=&amp;quot;white&amp;quot;) +
    ylim(-4,4) + facet_wrap(~state)
stored&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-03-07-pew-data-on-bond-ratings-and-rainy-day-funds/index_files/figure-html/Plot5c-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-panel-data-properties&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Panel Data Properties&lt;/h2&gt;
&lt;p&gt;Panel data estmators for linear problems benefit from a very useful decomposition from ANOVA. The total variation in a variable can be decomposed into two components: a within-unit or short-run component and a between-unit averages component (that is constant for any given unit). It is always important, as emphasised in the modelling in Mundlak (1977), to consider the variance components because they conttribute insights into the nature of inferences by telling us how much information and of what sort is contained in each indicator. The number of controls in the study is manageable so in depth analysis is possible.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;analysing-the-scaled-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Analysing the Scaled Data&lt;/h2&gt;
&lt;p&gt;With continuous measures on the response imputed over 10,000 samples, we can turn to an analysis of these samples to reexamine the dynamics of rainy day fund expenditures on bond ratings.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(haven)
nlswork &amp;lt;- read_stata(&amp;quot;http://www.stata-press.com/data/r13/nlswork.dta&amp;quot;)
library(dplyr)
XTSUM &amp;lt;- function(data, varname, unit) {
  varname &amp;lt;- enquo(varname)
  unit &amp;lt;- enquo(unit)
  ores &amp;lt;- nlswork %&amp;gt;% summarise(ovr.mean=mean(!! varname, na.rm=TRUE), ovr.sd=sd(!! varname, na.rm=TRUE), ovr.min = min(!! varname, na.rm=TRUE), ovr.max=max(!! varname, na.rm=TRUE), N.overall=sum(as.numeric(!is.na(!! varname))))
  bmeans &amp;lt;- nlswork %&amp;gt;% group_by(!!unit) %&amp;gt;% summarise(meanx=mean(!! varname, na.rm=T), t.count=sum(as.numeric(!is.na(!! varname))))
  bres &amp;lt;- bmeans %&amp;gt;% summarise(between.sd = sd(meanx, na.rm=TRUE), between.min = min(meanx, na.rm=TRUE), between.max=max(meanx, na.rm=TRUE), t.bar=mean(t.count, na.rm=TRUE), Groups=n())
  wdat &amp;lt;- nlswork %&amp;gt;% group_by(!!unit) %&amp;gt;% mutate(W.x = scale(!! varname, scale=FALSE))
  wres &amp;lt;- wdat %&amp;gt;% ungroup() %&amp;gt;% summarise(within.sd=sd(W.x, na.rm=TRUE), within.min=min(W.x, na.rm=TRUE), within.max=max(W.x, na.rm=TRUE))
  return(list(ores=ores,bres=bres,wres=wres))
}
XTSUM(nlswork, varname=hours, unit=idcode)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $ores
## # A tibble: 1 x 5
##   ovr.mean ovr.sd ovr.min ovr.max N.overall
##      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;     &amp;lt;dbl&amp;gt;
## 1     36.6   9.87       1     168     28467
## 
## $bres
## # A tibble: 1 x 5
##   between.sd between.min between.max t.bar Groups
##        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;int&amp;gt;
## 1       7.85           1        83.5  6.04   4711
## 
## $wres
## # A tibble: 1 x 3
##   within.sd within.min within.max
##       &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
## 1      7.52      -38.7       93.5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Longitudinal Panel Data R Packages</title>
      <link>/2018/02/24/panel-data-r-packages/</link>
      <pubDate>Sat, 24 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/2018/02/24/panel-data-r-packages/</guid>
      <description>


&lt;div id=&#34;longitudinal-and-panel-data-analysis-in-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Longitudinal and Panel Data Analysis in R&lt;/h1&gt;
&lt;p&gt;Goal: A CRAN task view for panel/longitudinal data analysis in R.&lt;/p&gt;
&lt;div id=&#34;what-is-panel-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is Panel Data?&lt;/h2&gt;
&lt;p&gt;Panel data are variously called longitudinal, panel, cross-sectional time series, and pooled time series data. The most precise definition is two-dimensional data; invariably one of the dimensions is time. We can think about a general depiction of what a model with linear coefficients typical for such data structures, though ridiculously overparameterized, like so:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_{it} = \alpha_{it} + X_{it}\beta_{it} + \epsilon_{it} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For example, suppose that we have a set of countries, or US states, municipalities in a state, or even individual people, indexed by &lt;span class=&#34;math inline&#34;&gt;\(i \in N\)&lt;/span&gt;. We observe those data at &lt;span class=&#34;math inline&#34;&gt;\(t \in T\)&lt;/span&gt; points in time. To be clear, a standard cross-section of data in this notation suppresses &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; because there is no observed over time variation. A single time series suppresses the &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. Of course, &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; are greater than 1.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;common-transformations-anova&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Common transformations: ANOVA&lt;/h2&gt;
&lt;p&gt;There are common transformations undertaken on the data prior to analysis. For example, the &lt;em&gt;within&lt;/em&gt; transformation that underlies a &lt;em&gt;fixed effects&lt;/em&gt; regression model transforms each vector of data (for &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;) as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ x^{W}_{it} = x_{it} - \overline{x}_{i} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The within-transformed data are of identical dimensions to the untransformed data. The other common transformation isolates the &lt;span class=&#34;math inline&#34;&gt;\(n-vector\)&lt;/span&gt; of unit means – the so-called &lt;em&gt;between&lt;/em&gt; data. Because the dimensions are non-overlapping/orthogonal; the key result in the applied statistics shows that the total variation in a variable &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is the sum of the &lt;em&gt;within&lt;/em&gt; variance and the &lt;em&gt;between&lt;/em&gt; variance.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;some-basic-summary&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Some Basic Summary&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;dplyr&lt;/em&gt; makes the basic summary and presentation quite straightforward. &lt;code&gt;plm&lt;/code&gt; contains a few datasets; I will choose an excerpt from Summers and Heston’s Penn World Tables – &lt;em&gt;SumHes&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(plm)
library(tidyverse)
library(dplyr)
data(SumHes)
summary(SumHes)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       year              country      opec       com            pop         
##  Min.   :1960   ALGERIA     :  26   no :3146   no :3120   Min.   :     42  
##  1st Qu.:1966   ANGOLA      :  26   yes: 104   yes: 130   1st Qu.:   2266  
##  Median :1972   BENIN       :  26                         Median :   5919  
##  Mean   :1972   BOTSWANA    :  26                         Mean   :  29436  
##  3rd Qu.:1979   BURKINA FASO:  26                         3rd Qu.:  18529  
##  Max.   :1985   BURUNDI     :  26                         Max.   :1051013  
##                 (Other)     :3094                                          
##       gdp              sr       
##  Min.   :  257   Min.   :-4.50  
##  1st Qu.:  942   1st Qu.: 8.90  
##  Median : 1957   Median :16.30  
##  Mean   : 3400   Mean   :16.91  
##  3rd Qu.: 4640   3rd Qu.:24.10  
##  Max.   :16570   Max.   :45.50  
## &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To actually summarize the data in an appropriate fashion, we will use &lt;em&gt;dplyr&lt;/em&gt; and &lt;em&gt;summarise&lt;/em&gt; after grouping the data by &lt;em&gt;country&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(rlang)
library(haven)
nlswork &amp;lt;- read_stata(&amp;quot;http://www.stata-press.com/data/r13/nlswork.dta&amp;quot;)
nlswork2 &amp;lt;- nlswork
nlswork2$race &amp;lt;- as.character(nlswork$race)
xtsum &amp;lt;- function(formula, data) {
  require(rlang)
  require(tidyverse)
  pform &amp;lt;- terms(formula, data=data)
  unit &amp;lt;- pform[[2]]
  vars &amp;lt;- attr(pform, &amp;quot;term.labels&amp;quot;)
  cls &amp;lt;- sapply(data, class)
  data &amp;lt;- data %&amp;gt;% select(which(cls%in%c(&amp;quot;numeric&amp;quot;,&amp;quot;integer&amp;quot;)))
  varnames &amp;lt;- intersect(names(data),vars)
  sumfunc &amp;lt;- function(data=data, varname, unit) {
  loc.unit &amp;lt;- enquo(unit)
  varname &amp;lt;- ensym(varname)
    ores &amp;lt;- data %&amp;gt;% filter(!is.na(!! varname)==TRUE) %&amp;gt;% summarise(
    O.mean=round(mean(`$`(data, !! varname), na.rm=TRUE), digits=3),
    O.sd=round(sd(`$`(data, !! varname), na.rm=TRUE), digits=3), 
    O.min = min(`$`(data, !! varname), na.rm=TRUE), 
    O.max=max(`$`(data, !! varname), na.rm=TRUE), 
    O.SumSQ=round(sum(scale(`$`(data, !! varname), center=TRUE, scale=FALSE)^2, na.rm=TRUE), digits=3), 
    O.N=sum(as.numeric((!is.na(`$`(data, !! varname))))))
 bmeans &amp;lt;- data %&amp;gt;% filter(!is.na(!! varname)==TRUE) %&amp;gt;% group_by(!! loc.unit) %&amp;gt;% summarise(
   meanx=mean(`$`(.data, !! varname), na.rm=T), 
   t.count=sum(as.numeric(!is.na(`$`(.data, !! varname)))))
  bres &amp;lt;- bmeans %&amp;gt;% ungroup() %&amp;gt;% summarise(
    B.sd = round(sd(meanx, na.rm=TRUE), digits=3),
    B.min = min(meanx, na.rm=TRUE), 
    B.max=max(meanx, na.rm=TRUE), 
    Units=sum(as.numeric(!is.na(t.count))), 
    t.bar=round(mean(t.count, na.rm=TRUE), digits=3))
  wdat &amp;lt;- data %&amp;gt;% filter(!is.na(!! varname)==TRUE) %&amp;gt;% group_by(!! loc.unit) %&amp;gt;% mutate(
    W.x = scale(`$`(.data,!! varname), scale=FALSE))
  wres &amp;lt;- wdat %&amp;gt;% ungroup() %&amp;gt;% summarise(
    W.sd=round(sd(W.x, na.rm=TRUE), digits=3), 
    W.min=min(W.x, na.rm=TRUE), 
    W.max=max(W.x, na.rm=TRUE), 
    W.SumSQ=round(sum(W.x^2, na.rm=TRUE), digits=3))
    W.Ratio &amp;lt;- round(wres$W.SumSQ/ores$O.SumSQ, digits=3)
  return(c(ores,bres,wres,Within.Ovr.Ratio=W.Ratio))
  }
res1 &amp;lt;- sapply(varnames, function(x) {sumfunc(data, !!x, !!unit)})
return(t(res1))
}  
xtsum(idcode~., data=nlswork2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          O.mean O.sd   O.min O.max    O.SumSQ  O.N   B.sd   B.min B.max   
## year     77.959 6.384  68    88       1162831  28534 5.157  68    88      
## birth_yr 48.085 3.013  41    54       258999.4 28534 3.052  41    54      
## age      29.045 6.701  14    46       1279992  28510 5.486  14    45      
## msp      0.603  0.489  0     1        6827.437 28518 0.398  0     1       
## nev_mar  0.23   0.421  0     1        5045.599 28518 0.368  0     1       
## grade    12.533 2.324  0     18       154082.7 28532 2.567  0     18      
## collgrad 0.168  0.374  0     1        3989.224 28534 0.405  0     1       
## not_smsa 0.282  0.45   0     1        5781.348 28526 0.411  0     1       
## c_city   0.357  0.479  0     1        6549.949 28526 0.427  0     1       
## south    0.41   0.492  0     1        6898.155 28526 0.467  0     1       
## ind_code 7.693  2.994  1     12       252718.4 28193 2.543  1     12      
## occ_code 4.778  3.065  1     13       266984.6 28413 2.865  1     13      
## union    0.234  0.424  0     1        3452.712 19238 0.334  0     1       
## wks_ue   2.548  7.294  0     76       1214713  22830 5.181  0     76      
## ttl_exp  6.215  4.652  0     28.88461 617516.8 28534 3.724  0     24.7062 
## tenure   3.124  3.751  0     25.91667 395453.2 28101 2.797  0     21.16667
## hours    36.56  9.87   1     168      2772858  28467 7.847  1     83.5    
## wks_work 53.989 29.032 0     104      23457236 27831 20.645 0     104     
## ln_wage  1.675  0.478  0     5.263916 6521.884 28534 0.425  0     3.912023
##          Units t.bar W.sd  W.min      W.max     W.SumSQ  Within.Ovr.Ratio
## year     4711  6.057 5.138 -14.16667  14.75     753323.2 0.648           
## birth_yr 4711  6.057 0     0          0         0        0               
## age      4710  6.053 5.169 -14.25     14.75     761852.2 0.595           
## msp      4711  6.053 0.324 -0.9333333 0.9333333 2991.619 0.438           
## nev_mar  4711  6.053 0.246 -0.9333333 0.9333333 1720.909 0.341           
## grade    4709  6.059 0     0          0         0        0               
## collgrad 4711  6.057 0     0          0         0        0               
## not_smsa 4711  6.055 0.183 -0.9285714 0.9333333 959.921  0.166           
## c_city   4711  6.055 0.249 -0.9333333 0.9333333 1768.61  0.27            
## south    4711  6.055 0.16  -0.9333333 0.9333333 728.353  0.106           
## ind_code 4695  6.005 1.708 -9.2       9.428571  82284.83 0.326           
## occ_code 4699  6.047 1.65  -10.3      10.66667  77374.88 0.29            
## union    4150  4.636 0.267 -0.9166667 0.9166667 1369.971 0.397           
## wks_ue   4645  4.915 6.054 -36.5      61.83333  836703.7 0.689           
## ttl_exp  4711  6.057 3.484 -15.85799  14.1656   346367.2 0.561           
## tenure   4699  5.98  2.66  -17.40278  12.5      198792.1 0.503           
## hours    4710  6.044 7.521 -38.71429  93.5      1610069  0.581           
## wks_work 4686  5.939 23.97 -72.42857  77.16667  15990016 0.682           
## ln_wage  4711  6.057 0.293 -2.082629  3.108762  2443.848 0.375&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The previous command takes a panel dataset organized by the unit and decomposes the within and between variance and standard deviation, etc. To summarize an individual variable in the same fashion, the command XTSUM below should accomplish the task.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;XTSUM &amp;lt;- function(data, varname, unit) {
  varname &amp;lt;- enquo(varname)
  unit &amp;lt;- enquo(unit)
  ores &amp;lt;- nlswork %&amp;gt;% summarise(ovr.mean=mean(!! varname, na.rm=TRUE), ovr.sd=sd(!! varname, na.rm=TRUE), ovr.min = min(!! varname, na.rm=TRUE), ovr.max=max(!! varname, na.rm=TRUE))
  bmeans &amp;lt;- nlswork %&amp;gt;% group_by(!!unit) %&amp;gt;% summarise(meanx=mean(!! varname, na.rm=T))
  bres &amp;lt;- bmeans %&amp;gt;% summarise(between.sd = sd(meanx, na.rm=TRUE), between.min = min(meanx, na.rm=TRUE), between.max=max(meanx, na.rm=TRUE), Groups=n())
  wdat &amp;lt;- nlswork %&amp;gt;% group_by(!!unit) %&amp;gt;% mutate(W.x = scale(!! varname, scale=FALSE))
  wres &amp;lt;- wdat %&amp;gt;% ungroup() %&amp;gt;% summarise(within.sd=sd(W.x, na.rm=TRUE), within.min=min(W.x, na.rm=TRUE), within.max=max(W.x, na.rm=TRUE))
  return(list(ores=ores,bres=bres,wres=wres))
}
XTSUM(nlswork, varname=hours, unit=idcode)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $ores
## # A tibble: 1 x 4
##   ovr.mean ovr.sd ovr.min ovr.max
##      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
## 1     36.6   9.87       1     168
## 
## $bres
## # A tibble: 1 x 4
##   between.sd between.min between.max Groups
##        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;  &amp;lt;int&amp;gt;
## 1       7.85           1        83.5   4711
## 
## $wres
## # A tibble: 1 x 3
##   within.sd within.min within.max
##       &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
## 1      7.52      -38.7       93.5&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;models-for-panel-data-standard-models-in-plm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Models for panel data: Standard models in &lt;em&gt;plm()&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;The workhorse package/library for the basic panel data models in R is &lt;em&gt;plm&lt;/em&gt;. The &lt;em&gt;JStatSoft&lt;/em&gt; &lt;a href=&#34;https://cran.r-project.org/web/packages/plm/vignettes/plm.pdf&#34;&gt;article was modified to become the vignette&lt;/a&gt; and is quite detailed, though dated. The previous equation can be simplified and detailed to derive the commonly deployed models.&lt;/p&gt;
&lt;div id=&#34;a-pooled-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Pooled Regression&lt;/h3&gt;
&lt;p&gt;Simply suppresses all subscripts on &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; to write:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_{it} = \alpha + X_{it}\beta + \epsilon_{it} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-within-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Within Regression&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ y_{it} = \alpha + X_{it}\beta^{W} + \epsilon^{W}_{it} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-between-regression&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A Between Regression&lt;/h3&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \overline{y}_{i} = \alpha + \overline{X}_{i}\beta^{B} + \epsilon^{B}_{i} \]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;models-with-heterogeneous-time-trends-phtt&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Models with heterogeneous time trends: &lt;em&gt;phtt()&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;An &lt;a href=&#34;https://arxiv.org/pdf/1407.6484.pdf&#34;&gt;article&lt;/a&gt; on panel data analysis with heterogeneous time trends appears in &lt;em&gt;JStatSoft&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multi-state-models-in-continuous-time-msm&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multi-state models in continuous time &lt;em&gt;msm()&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;The &lt;em&gt;msm&lt;/em&gt; library fits homogenous and inhomogenous Markov models for multinomial outcomes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;latent-markov-models-with-lmest&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Latent Markov models with &lt;em&gt;LMest()&lt;/em&gt;&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;LM models are designed to deal with univariate and multivariate longitudinal data based on the repeated observation of a panel of subjects across time. More in detail, LM models are specially tailored to study the evolution of an individual characteristic of interest that is not directly observable. This characteristic is represented by a latent process following a Markov chain as in a Hidden Markov (HM) model (Zucchini and MacDonald, 2009). These models also allow us to account for time-varying unobserved heterogeneity in addition to the effect of observable covariates on the response variables.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;a href=&#34;https://arxiv.org/pdf/1501.04448.pdf&#34;&gt;preprint&lt;/a&gt; is here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cquad&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;&lt;em&gt;cquad&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;In the study of non-linear panel data models, unobserved heterogeneity and state dependence are hardest with the least information – binary outcomes. The &lt;em&gt;cquad&lt;/em&gt; package for Stata and R utilizes an approach by Bartolucci and Nigro described in &lt;a href=&#34;https://www.jstatsoft.org/article/view/v078i07/v78i07.pdf&#34;&gt;Bartolucci and Pigini&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
