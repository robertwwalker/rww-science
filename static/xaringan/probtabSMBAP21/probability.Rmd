---
title: "Tidy, Probability and Tables"
subtitle: "The Core of Statistical Thinking"
author: "Robert W. Walker"
institute: "Atkinson Graduate School of Management"
date: "2020/02/01 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false

---
# Tidy thinking

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(kableExtra)
library(xaringanthemer)
style_solarized_dark(
#  base_color = "#1c5253",
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono")
)
```


First, import my data.

```{r, eval=FALSE}
library(tidyverse)
library(readxl)
Bonds <- read_excel("static/xaringan/probtabSMBAP21/data/Week-2.xlsx", sheet = "Bonds")
CEOComp <- read_excel("static/xaringan/probtabSMBAP21/data/Week-2.xlsx", sheet = "CEOComp")
FastFood <- read_excel("static/xaringan/probtabSMBAP21/data/Week-2.xlsx", sheet = "FastFood", na="NA")
DisabilityExp <- read_excel("static/xaringan/probtabSMBAP21/data/Week-2.xlsx", sheet = "DisabilityExp")
UCBAdmit <- read_excel("static/xaringan/probtabSMBAP21/data/Week-2.xlsx", sheet = "UCBAdmit")
```

---
# Easier

```{r}
load(url("https://github.com/robertwwalker/DADMStuff/raw/master/Week3Data.RData"))
```

---
# tidy

---
# Scoping

That operator `$` is the first encounter with the `scope` of something.  We are trying to pull `protein` from `FastFood`.  There are other basic operators in $R$ to accomplish the same thing.  We could have asked for the [even less typing-efficient] relevant row and column definition with:

```{r}
FastFood[,"protein"]
```

---
# Piping [%>%]
If I want more than one, this becomes cumbersome.  The tidyverse, built around a `piping operator` -- `%>%` --, was developed as a solution to a data.frame centric form of analysis.  Here's how it works.  We start with data and `pipe` it so that the names are understood in the context of the data that we begin with.  The main initial helper that we will make use of is `skim` from the `skimr` library.

---
# A Skim of the Data

```{r, eval=FALSE}
library(skimr)
FastFood %>% skim()
```

---

<small>
```{r, echo=FALSE, results='markup'}
library(skimr)
FastFood %>% skim() %>% kable() %>% kable_styling() %>%
  scroll_box(width = "800px", height = "500px")
```
</small>

---
# dplyr verbs

There are four main `dplyr` verbs that we will play with, and some helpers. 
- filter   

--
- select   

--
- mutate   

--
- summarize or summarise  


---
class: center

# `filter()`


---
# filter [1 of 3]

`filter` selects rows according to some set of conditions.  
- Valid with `==`
- Can use `%in%` with a vector `c()`.


```{r}
FastFood %>% filter(restaurant == "Taco Bell")
FastFood %>% filter(restaurant %in% c("Taco Bell","Burger King"))
FastFood %>% filter(startsWith(restaurant,"S")==TRUE) %>% group_by(restaurant) %>% skim()
```


---
# filter [2 of 3]

`filter` selects rows according to some set of conditions.  
- Can use `%in%` with a vector `c()`.

<small>
```{r}
FastFood %>% filter(restaurant%in%c("Taco Bell","Burger King"))
```
</small>

---
# filter [3 of 3]

`filter` selects rows according to some set of conditions.  
- Or more elaborate combinations of elements
- It has to return a logical [TRUE/FALSE] to filter the rows.

---

<small>
```{r, results='markup'}
FastFood %>% filter(startsWith(restaurant,"S")==TRUE) %>% group_by(restaurant) %>% skim() %>% kable() %>% kable_styling() %>%
  scroll_box(width = "100%", height = "500px")
```
</small>


---
# Inversion with !

<small>
```{r, results='markup'}
FastFood %>% filter(!(startsWith(restaurant,"S")==TRUE)) %>% group_by(restaurant) %>% skim() %>% kable() %>% kable_styling() %>%
  scroll_box(width = "100%", height = "500px")
```
</small>


---
class: center, middle

# `select()`


---
class: inverse
# select [1/2]

`select` selects columns according to some set of names/conditions.  

<small>
```{r}
FastFood %>% select(restaurant, calories)
```

---
class: inverse
# select [2/2]

`select` selects columns according to some set of names/conditions.  

<small>
```{r}
FastFood %>% select(restaurant,starts_with("vit"))
```

---
class: inverse
# select [2/2]

`select` selects columns according to some set of names/conditions.  Negative selection can occur.

<small>
```{r}
FastFood %>% select(-restaurant)
```



---
class: center, middle

# `mutate()`


---
# mutate [and transmute]

`mutate()` and `transmute()` are the core method for adding variables [columns] to existing data.  The key difference is that `mutate()` retains existing variables while `transmute()` drops them.  Let's see it for sodium, rescaled to grams.

*mutate* will keep all columns.

<small>
```{r}
FastFood %>% 
  mutate(Sodium.Grams = sodium / 1000) %>%
  select(restaurant,Sodium.Grams,sodium,everything())
```
</small>


---
class: center, middle

# `transmute()`


---
# transmute

*transmute* will only keep the called columns.

<small>
```{r}
FastFood %>% transmute(Sodium.Grams = sodium / 1000)
# To keep a variable, copy it.
# FastFood %>% transmute(restaurant = restaurant, Sodium.Grams = sodium / 1000)
```
</small>

---
# NB: Reassigning or newly assigning

To make these `mutate()` a part of the data, we assign it a *new name* or reassign it.

<small>
```{r}
FastFood <- FastFood %>% mutate(Sodium.Grams = sodium / 1000)
My.Fast.Food <- FastFood %>% mutate(Sodium.Grams = sodium / 1000)
```

---

# `mutate()` and `transmute()` can be fancy
## Fixing a Frustration and a little Visual

Virtually all of these functions can embed other functions.  We can use mutate with functions to do pretty fancy things.  Let me isolate the chicken items.

```{r}
FastFood <- FastFood %>% mutate(Chicken = stringr::str_detect(item, 'Chicken|Chick-n'))
```

---

# What's the distribution of Chicken items by chain?

```{r, eval=FALSE}
ggplot(FastFood) +
 aes(x = restaurant, fill = Chicken) +
 geom_bar(position = "dodge") +
 coord_flip() +
 theme_minimal() +
 labs(x="", y="Menu Items", title="Chicken Menu Items by Fast Food Chain")
```


---

```{r, echo=FALSE}
ggplot(FastFood) +
 aes(x = restaurant, fill = Chicken) +
 geom_bar(position = "dodge") +
 coord_flip() +
 theme_minimal() +
 labs(x="", y="Menu Items", title="Chicken Menu Items by Fast Food Chain")
```




---
class: center, middle

# `group_by()`


---
# The Magic of `group_by`

`group_by` is a core `tidyverse` operator for repeating something by groups.  By itself, it simply splits a data object according to the grouping variable(s).

--

**But that is exactly what a pivot table does.**

---
# Grouping and pipes

<small>
```{r, results='markup'}
FastFood %>% group_by(restaurant) %>% skim() %>% kable() %>% kable_styling() %>%
  scroll_box(width = "100%", height = "500px")
```
</small>


---
# A Two Variable Pivot

<small>
```{r, results='markup'}
FastFood %>% group_by(restaurant,Chicken) %>% skim(Sodium.Grams) %>% kable() %>% kable_styling() %>% scroll_box(width = "100%", height = "50%")
```
</small>

---
# summarise / summarize

Is the analog to creating a pivot table in R by whatever groupings we wish.

```{r}
FastFood %>% group_by(restaurant) %>% summarise(Mean.Protein = mean(protein), Mean.Protein.NA = mean(protein, na.rm=TRUE))
```

---
# summarise

Is the analog to creating a pivot table in R by whatever groupings we wish.

```{r}
FastFood %>% group_by(restaurant, Chicken) %>% summarise(Mean.Protein = mean(protein), Mean.Protein.NA = mean(protein, na.rm=TRUE))
```


---
# ungroup()

We need `ungroup()` when we want to combine `mutate()` and `group_by()` to calculate aggregate statistics for all relevant rows.  Objects retain their `grouped` status unless we actively remove it.

<small>
```{r}
FastFood <- FastFood %>% 
  group_by(restaurant) %>% 
  mutate(Avg.Protein = mean(protein, na.rm=TRUE), Protein.Dev = protein - Avg.Protein) %>%
  ungroup()
```
</small>

---
class: center, middle

# `arrange()`


---
# arrange() [1/2]

We can use arrange to sort a result.  For example,

```{r}
FastFood %>% 
  group_by(restaurant) %>% 
  summarise(Avg.Calories = mean(calories)) %>% 
  arrange(Avg.Calories)
```

---
# arrange() [2/2]

We can use arrange to sort a result, and `desc()` to flip it.  For example,

```{r}
FastFood %>% 
  group_by(restaurant) %>% 
  summarise(Avg.Calories = mean(calories)) %>% 
  arrange(desc(Avg.Calories))
```

---
# A Basic Table: Counts

```{r}
( Restaurant.Table <- FastFood %>% group_by(restaurant) %>% summarise(Count = n()) %>% arrange(Count) )
```

---
# A More Elaborate Table: Counts

```{r, message=FALSE}
( Rest.Chicken.Table <- FastFood %>% group_by(restaurant, Chicken) %>% summarise(Count = n()) )
```


---
# A First Data Visualisation

```{r}
FastFood %>% group_by(restaurant) %>% summarise(Count = n()) %>% ggplot() + aes(x=restaurant, y=Count) + geom_col()
```

---
# Adding in Chicken

```{r, message=FALSE}
FastFood %>%  group_by(restaurant, Chicken) %>% summarise(Count = n()) %>%
    ggplot() + aes(x=restaurant, y=Count, fill=Chicken) + geom_col() #<<
```

---
# More Chaining [`fct_reorder()`]

```{r}
FastFood %>% group_by(restaurant) %>% summarise(Count = n()) %>% ggplot() + aes(x=fct_reorder(restaurant, Count), y=Count) + geom_col() + labs(x="Chain", y="Count") + coord_flip() 
```

---
# Even More Chaining

```{r, fig.height=6}
FastFood %>% group_by(restaurant) %>% summarise(Count = n()) %>% ggplot() + aes(x=fct_reorder(restaurant, desc(Count)), y=Count) + geom_col() + labs(x="Chain", y="Count") + coord_flip() 
```

---
class: inverse

# A Note on Skim

We could do it by hand.

```{r}
FastFood %>% group_by(restaurant) %>% 
  summarise(Mean = mean(calories, na.rm=TRUE), 
            SD = sd(calories, na.rm=TRUE), 
            Min = min(calories, na.rm=TRUE), 
            Median = median(calories, na.rm=TRUE), 
            Max = max(calories, na.rm=TRUE), 
            Q1 = quantile(calories, 0.25, na.rm=TRUE), 
            Q3 = quantile(calories, 0.75, na.rm=TRUE))
```

---
class: inverse

# A Recap

.pull-left[
**Four `dplyr` verbs:**  
- `filter()`  
- `select()`  
- `mutate()` / `transmute()`  
- `summarise()`  
]

.pull-right[
**Two helpers:**  
- `group_by()` and `ungroup()`  
- `arrange()` and `desc()`  
]

---
class: inverse, center

# Probability and Tables


---
class: inverse, center

# Probability

```{r, echo=FALSE}
library(png); library(here); library(kableExtra)
Prob.Def <- readPNG(here("probtabSMBAP21/data","ETJ-Probability.png"))
grid::grid.raster(Prob.Def)
```


---
class: inverse

# Probability

.font150[
Two rules:
1. Probabilities sum to one.
2. The probability of any event is greater than or equal to zero.
]
---
class: inverse

# Where does Probability Come From?

There are three common sources of probabilities:

--

- Known formula [Dice, Coins, etc.]

--

- Empirical frequency

--

- Subjective belief


---
class: inverse

# A priori probability

The probability of a given integer on a k-sided die: $\frac{1}{k}$.

The probability of **heads** with a fair coin: $\frac{1}{2}$.

The probability of a Queen?  $\frac{4}{52}$

The probability of a Diamond?  $\frac{13}{52}$

The Queen of Diamonds? $\frac{1}{52}$ or   $(\frac{4}{52}\times\frac{13}{52})$

**Quasirandom numbers**

---
class: inverse

# Empirical probability: frequency

How often does something happen?

<iframe width="560" height="315" src="https://www.youtube.com/embed/tgC-vZp07YM"></iframe>


---
class: inverse

# This is Historical Statistics

How likely am I to be admitted? *Consult the admissions rate*

How fast do I drive? *Likelihood of law enforcement and need for speed*

In data: this is tables.

---
# Berkeley

.pull-left[
```{r MD1, echo=FALSE}
library(tidyverse)
library(janitor)
table(UCBAdmit$M.F,UCBAdmit$Admit)
UCBAdmit %>% tabyl(M.F, Admit)
```
]

.pull-right[
```{r MD2, eval=FALSE}
library(tidyverse)
library(janitor)
table(UCBAdmit$M.F,UCBAdmit$Admit)
UCBAdmit %>% tabyl(M.F, Admit)
```
]

---
class:inverse

# Three Versions

.pull-left[
```{r MD1B}
prop.table(table(UCBAdmit$M.F,UCBAdmit$Admit), 1)
prop.table(table(UCBAdmit$M.F,UCBAdmit$Admit), 2)
prop.table(table(UCBAdmit$M.F,UCBAdmit$Admit))
```
]

.pull-right[
```{r MD2B}
UCBAdmit %>% tabyl(M.F, Admit) %>% adorn_percentages("row")
UCBAdmit %>% tabyl(M.F, Admit) %>% adorn_percentages("col")
UCBAdmit %>% tabyl(M.F, Admit) %>% adorn_percentages("all")
```
]

```{r, echo=FALSE}
UCBM <- ggplot(UCBAdmit) + aes(x=M.F, fill=Admit) + geom_bar(position="dodge") + scale_fill_viridis_d()
ggsave("./UCBM.png", height=8, units="in", device="png")
```

---
background-image: url("UCBM.png")
background-size: contain
class: bottom

# Plot It

```{r, eval=FALSE}
( UCBM <- ggplot(UCBAdmit) + aes(x=M.F, fill=Admit) + geom_bar(position="dodge") + scale_fill_viridis_d() )
```

More on this later.

---
class: inverse

# Subjective Probability

How likely **do we believe** something is?

---
class: inverse

# The Great Divide

Empirical frequency vs. subjective belief

---
class: inverse

# Empirical Frequency: She's Right  

## Physics Disagrees: We Goin Nova.....

--

## Annie's a liar.

What matters in group decision making is probably as much the beliefs [subjective] as the evidence [frequency].

How should we reflect this in strategies of argumentation/persuasion?

---
class: inverse

```{r, eval=TRUE}
# RUN ME
# may need to install.packages("countdown")
library(countdown)
countdown_fullscreen(
  minutes = 5, seconds = 0,
  margin = "5%",
  font_size = "8em",
)
```

---
# Three Concepts from Set Theory

- Intersection [and]
- Union [or] **avoid double counting the intersection**
- Complement [not]

---

# Three Distinct Probabilities

- Joint: Pr(x=x and y=y)
- Marginal: Pr(x=x) or Pr(y=y)
- Conditional: Pr(x=x | y=y) or Pr(y =y | x = x)

---

# Joint Probability

**The table sums to one**.  

For Berkeley:

```{r}
UCBAdmit %>% tabyl(M.F, Admit) %>% adorn_percentages("all")
prop.table(table(UCBAdmit$M.F,UCBAdmit$Admit))
```

---
# Marginal Probability

**The row/column sums to one**.  We collapse the table to a single margin.  Here, two can be identified.  The probability of Admit and the probability of M.F.  

.pull-left[
```{r}
UCBAdmit %>% tabyl(M.F)
UCBAdmit %>% tabyl(Admit)
```

]

.pull-right[
```{r}
prop.table(table(UCBAdmit$M.F))
prop.table(table(UCBAdmit$Admit))
```
]

---
# Conditional Probability

How does one margin of the table break down given values of another?  **Each row or column sums to one**  

Four can be identified, the probability of admission/rejection for Male, for Female; the probability of male or female for admits/rejects.

For Berkeley:

.pull-left[
```{r}
UCBAdmit %>% tabyl(M.F, Admit) %>% adorn_percentages("row")
prop.table(table(UCBAdmit$M.F,UCBAdmit$Admit), 1)
```
]

.pull-right[
```{r}
UCBAdmit %>% tabyl(M.F, Admit) %>% adorn_percentages("col")
prop.table(table(UCBAdmit$M.F,UCBAdmit$Admit), 2)
```
]

---
# Law of Total Probability

Is a combination of the distributive property of multiplication and the fact that probabilities sum to one.

For example, the probability of Admitted and Male is the probability of admission for males times the probability of male.

$$ Pr(x=x, y=y) = Pr(y | x)Pr(x) $$

Or it is the probability of being admitted times the probabilty of being male among admits.

$$ Pr(x=x, y=y) = Pr(x | y)Pr(y) $$

---

# Now the Substance

.pull-left[
The `ggplot` fill aesthetic is great for displaying these things.   For example, are males and females equally likely to be admitted to Berkeley?

**Plaintiffs say no.**

```{r, echo=TRUE, eval=FALSE}
ggplot(UCBAdmit) + aes(x=M.F, fill=Admit) + geom_bar() + scale_fill_viridis_d()
```
]

.pull-right[
```{r, echo=FALSE}
ggplot(UCBAdmit) + aes(x=M.F, fill=Admit) + geom_bar() + scale_fill_viridis_d()
```
]



---

# Is that an Adequate Comparison?


.pull-left[
The University says no.  Why?  The most important factor in the probability of admission is likely to be the department.  This has a huge impact on what we see.


```{r, eval=FALSE}
ggplot(UCBAdmit) + 
  aes(x=M.F, fill=Admit) + 
  geom_bar(position="fill") + 
  scale_fill_viridis_d() + 
  facet_wrap(vars(Dept))
```
]

.pull-right[
```{r, eval=TRUE, echo=FALSE}
ggplot(UCBAdmit) + aes(x=M.F, fill=Admit) + geom_bar(position="fill") + scale_fill_viridis_d() + facet_wrap(vars(Dept))
```

]



---

# The Magic of Bayes Rule

To find the joint probability [the intersection] of x and y, we can use either of the aforementioned methods.  To turn this into a conditional probability, we simply take it is a proportion of the relevant margin.

$$ Pr(x | y) = \frac{Pr(y | x) Pr(x)}{Pr(y)} $$

By itself, this is algebra.  It is magic in an application.

$$ Pr(User | +) = \frac{Pr(+ | User) Pr(User)}{Pr(+)} $$

This poses the question: what does a positive test mean?

---
# Working an Example

Suppose a test is 99% accurate for Users and 95% accurate for non-Users.  Moreover, suppose that Users make up 10% of the population.  So given some population to which this applies, we have:

$Pr(User, +) = Pr(+ | User)*Pr(User)$

$Pr(User, -) = Pr(- | User)*Pr(User)$

$Pr(\overline{User}, +) = Pr(+ | \overline{User})*Pr(\overline{User})$

and

$Pr(\overline{User}, -) = Pr(- | \overline{User})*Pr(\overline{User})$



---
## The Table


 Status  |  Positive | Negative | Total
---------|-----------|----------|------
 User    | 0.099     | 0.001    | 0.1
non-User | 0.045     | 0.855    | 0.9
---------|-----------|----------|------
Total    | 0.144     | 0.856    | 1.0

$Pr(User | +) = \frac{Pr(+ | User) Pr(User)}{Pr(+)}$

yields:

$Pr(User | +) = \frac{0.099}{0.144} = 0.6875$


---
# Dichotomization is Prone to Error

As Jaynes suggests, probability is the extension of Aristotellian logic, but we are trying to go back to binary.
